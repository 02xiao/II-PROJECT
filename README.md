# II-PROJECT
自监督预训练 + PEFT 微调：轻量级GPT 垃圾短信分类器构建（从https://github.com/rasbt/LLMs-from-scratch学习体会）                                

<img width="415" height="292" alt="image" src="https://github.com/user-attachments/assets/afee97a2-b907-4c1d-82f6-d8fd99b9c588" />

一、项目介绍
1.1研究背景
近年来，近年来，以GPT系列为代表的大语言模型（LLM）在自然语言处理（NLP）领域取得了突破性进展。其成功的核心在于采用了“预训练-微调”（Pre-training and Fine-tuning）范式。首先，在海量的无标签文本数据上对模型进行预训练，使其学习到通用的语言知识；然后，在特定下游任务的少量有标签数据上进行微调，将通用知识适配于专门应用。本项目旨在完整复现这一经典范式，涵盖从预训练到微调的全过程。

1.2核心目标
本项目的核心目标是实践并验证针对特定分类任务的LLM微调技术。具体而言，本项目旨在：本项目的核心目标是实践并验证“预训练-微调”技术流程。具体而言，本项目旨在：
在一个小规模的无标签文本数据集上，从零开始预训练一个小型GPT模型，使其掌握基本的语言规律。选择一个更大、更强的公开预训练GPT-2模型作为微调阶段的基础模型。在一个公开的二元文本分类数据集（SMS垃圾短信分类）上，对该模型进行参数高效微调。实现一个高效的微调策略：仅更新模型的一小部分参数，而非训练整个模型，从而在保留模型泛化能力的同时，降低计算资源需求和训练时间。评估微调后模型的性能，并将其封装成一个可交互的Web应用，以直观展示其最终分类能力。
1.3所用到的AI技术
大语言模型（LLM）：采用基于Transformer架构的GPT-2 Small（124M参数）模型。该模型具备强大的文本理解和表征能力。
迁移学习与微调（Transfer Learning & Fine-tuning）：将GPT-2在通用语料上学到的知识，迁移到垃圾短信分类这一特定任务上。通过在目标任务数据上进行有监督训练，微调部分模型参数，使其适应新任务。
参数高效微调（Parameter-Efficient Fine-Tuning, PEFT）：为防止模型在小数据集上发生灾难性遗忘并提高训练效率，本项目冻结了大部分预训练参数，仅对与分类任务最相关的顶层网络层进行训练。
自然语言处理（NLP）：包括使用tiktoken进行高效分词、文本填充（Padding）与截断（Truncation）等数据预处理技术。
深度学习框架：使用PyTorch构建模型、定义损失函数（交叉熵损失）和优化器（AdamW），并完成整个训练流程。
二、实践过程
2.1语言模型的预训练
此步骤旨在模拟LLM知识获取的第一阶段，即在一个无标签文本语料上教会一个随机初始化的模型掌握语言本身。
数据集与目标任务：选用英文短篇小说《The Verdict》的纯文本文件作为训练语料。预训练的目标任务是下一个词预测。这是一种无监督学习，模型在没有任何人工标签的情况下，通过文本自身的上下文进行学习。
模型初始化：构建一个参数量较小的GPT模型。至关重要的是，该模型的权重被随机初始化，意味着在训练开始前，它不具备任何语言知识。
训练过程与损失函数：采用自回归的方式进行训练。模型接收一段文本序列作为输入，并被要求预测序列中每一个位置的下一个词。使用交叉熵损失函数来衡量模型预测的词与真实文本中的下一个词之间的差距。通过反向传播和AdamW优化器不断调整模型权重，以最小化这个差距。
预训练结果：经过多个周期的训练后，模型能够根据给定的前文（Context），生成符合小说风格和语法逻辑的连贯文本。模型已成功从原始文本中学习到了语言的内在规律。

<img width="203" height="228" alt="image" src="https://github.com/user-attachments/assets/169142ae-d449-4d8f-952c-af4499e0da6d" />
<img width="202" height="230" alt="image" src="https://github.com/user-attachments/assets/dea1a33f-ba43-4c1e-98c9-1d02fd81e714" />
	
2.2微调数据的准备与加载
步骤一：数据准备与加载。本项目选用“SMS Spam Collection”数据集，该数据集包含约5500条被标记为“ham”（正常）或“spam”（垃圾）的短信。
<img width="363" height="318" alt="image" src="https://github.com/user-attachments/assets/1189aa05-5d3b-4ee0-9945-03b15b1936c6" />

数据平衡处理：原始数据存在严重的类别不平衡问题。为避免模型偏向多数类，采用随机下采样策略，从正常短信中随机抽取与垃圾短信相同数量的样本，构建了一个类别均衡的数据集，使其包含每个类别的747个实例。
	
数据集划分：将均衡后的数据集按7:1:2的比例划分为训练集、验证集和测试集。
分词与序列化：由于这些文本消息的长度各不相同，如果我们想将多个训练样本合并到一个批次中，我们将所有消息填充至数据集或批次中最长消息的长度。使用与GPT-2兼容的tiktoken分词器对文本进行编码。为适应模型批处理的需求，对所有文本序列进行填充（Padding），使其长度统一为训练集中最长序列的长度（120个Tokens）。
<img width="415" height="172" alt="image" src="https://github.com/user-attachments/assets/9825e24b-48bf-43b2-85fe-b3d0a3955c8c" />

数据加载器：利用PyTorch的Dataset和DataLoader类，构建了用于模型训练和评估的数据管道。设计的 SpamDataset 类会识别训练数据集中的最长序列，并向其他序列添加填充标记，使其长度与该最长序列相匹配。在实际操作中还需要根据最长的训练序列对验证集和测试集进行了填充，长度超过最长训练样本的验证集和测试集样本会被encoded_text[:self.max_length] 截断。
<img width="361" height="423" alt="image" src="https://github.com/user-attachments/assets/559c8fcd-86c1-40a2-a7e4-c46ae7b75431" />

<img width="397" height="291" alt="image" src="https://github.com/user-attachments/assets/8cf2547e-ef1e-4a58-856a-05cf41553b67" />

作为验证步骤，我们遍历数据加载器，确保每个批次包含8个训练样本，其中每个训练样本包含120个标记。我们打印每个数据集中的批次总数如下。
<img width="206" height="56" alt="image" src="https://github.com/user-attachments/assets/679ca1c7-f643-47d4-ad87-ed7fa227e4a3" />
<img width="196" height="60" alt="image" src="https://github.com/user-attachments/assets/77f6072d-198b-4bd2-8bb7-d15a9c366bec" />

步骤二：模型构建与适配

加载预训练模型：加载拥有1.24亿参数的GPT-2 Small模型及其预训练权重。
参数冻结：默认情况下，模型的所有参数（requires_grad=False）均被冻结，以防止在微调过程中破坏其已有的语言知识。
修改分类头：GPT-2的原始输出层（out_head）用于预测词汇表中的下一个词。为适配二元分类任务，将其替换为一个新的线性层，输入维度为模型的嵌入维度（768），输出维度为类别数（2）。
<img width="403" height="314" alt="image" src="https://github.com/user-attachments/assets/72dc78d8-5553-4d87-93ac-55f6490500aa" />

选择性解冻：为提升性能，本项目解冻了最后一个Transformer块（trf_blocks[-1]）及最终的层归一化模块（final_norm）的参数。最后一个Transformer块的输出包含了对整个输入序列最丰富的语义信息，对其进行微调能更好地适应新任务。我们替换输出层，它原本将层输入映射到 50257 维（词汇表的大小）。由于我们对模型进行微调是为了进行二元分类（预测两类：“垃圾邮件”和“非垃圾邮件”），我们可以像下面这样替换输出层，该输出层默认情况下是可训练的。

我们也使最后一个 Transformer 模块和连接最后一个 Transformer 模块与输出层的最终 LayerNorm 模块可训练。
<img width="405" height="385" alt="image" src="https://github.com/user-attachments/assets/2311f022-4bb4-41ae-96ed-423e977f93c9" />

GPT类模型使用一种因果注意力掩码，该因果掩码使得当前标记仅关注其当前位置和前一个标记的位置。由于我们向模型输入了一个包含4个输入词元的文本样本，因此输出由 4 个二维输出向量组成。基于因果注意力机制，第 4 个（最后一个）词元包含的信息量最大，因为它是唯一一个包含了所有其他词元信息的词元。因此，我们对最后一个标记尤为感兴趣，我们将对其进行微调，以用于垃圾邮件分类任务。
<img width="404" height="409" alt="image" src="https://github.com/user-attachments/assets/d6e3575a-aa90-47e9-81ad-f0610c27a543" />

2.3模型训练与评估
训练过程：采用AdamW优化器和最小化交叉熵损失函数，在训练集上对模型进行5个周期的训练。我们定义并使用训练函数来提高模型的分类精度。下面这个 train_classifier_simple 函数可以跟踪已看到的训练样本数量（ examples_seen ），而不是已看到的词元数量。同时在每个epoch结束后计算准确率，而不是在每个epoch结束后打印示例文本。

<img width="415" height="341" alt="image" src="https://github.com/user-attachments/assets/87b4686a-1fad-4bec-a91a-310a807e27a0" />
<img width="415" height="434" alt="image" src="https://github.com/user-attachments/assets/bfcdf256-5a3c-43c9-967f-2248c658b0d2" />

我使用 matplotlib 绘制训练集和验证集的损失函数图。
<img width="203" height="120" alt="image" src="https://github.com/user-attachments/assets/aaa2489a-5889-4b4d-bd6f-b297e6cf639e" />
<img width="202" height="118" alt="image" src="https://github.com/user-attachments/assets/429f6ec0-54c5-4614-ab32-cca2604d8ee4" />

从左图的下降斜率可以看出，该模型学习效果良好。此外训练损失和验证损失非常接近，表明该模型没有过拟合训练数据的倾向。右图的准确度图我们可以看到，该模型在第4和第5个训练轮后都达到了相对较高的训练和验证准确率。
在每个周期结束后，使用验证集评估模型性能，以监控训练过程并防止过拟合。我们使用准确率作为核心评估指标。其中左图是未对模型进行微调前的不同数据集的分类准确率，右图是对模型微调后的分类准确率。可以发现。不管是训练集、验证集、还是测试集。分类准确率都是有着显著的提升。可见微调对于模型性能有着显著的积极作用。
	
实验结果：经过5个周期的微调，模型性能显著提升。训练损失和验证损失均稳定下降，而准确率则稳步上升并最终收敛。在测试集上的最终准确率达到了95.67%，远高于随机基线（50%），证明了微调策略的有效性。
<img width="198" height="45" alt="image" src="https://github.com/user-attachments/assets/e2921d20-6436-4a97-9a2c-1a01d7805798" />
<img width="192" height="46" alt="image" src="https://github.com/user-attachments/assets/d5afc878-61c2-4f1b-85c3-57abe6adeaef" />
2.4结果展示
构建与基于 GPT 的垃圾邮件分类器交互的用户界面。
<img width="416" height="240" alt="image" src="https://github.com/user-attachments/assets/bf35af0b-395c-41e5-adb6-cd6355e497a6" />
三、心得体会
3.1 实践中的难点与解决思路
难点1：如何让模型从零学会语言？通过自监督学习。通过“下一个词预测”这一巧妙的任务设计，模型可以从海量无标签文本中为自己创造出无穷无尽的“（上下文，目标词）”训练样本，从而完成知识的原始积累。
难点2：如何在有限资源下应用大模型？通过采用参数高效微调（PEFT）策略，我们无需从头训练一个巨大的模型，也无需更新其所有参数。而是站在巨人的肩膀上，仅微调与任务最相关的少数参数，就能以极低的成本将模型的通用能力适配到特定领域，实现了资源效率和模型性能的最佳平衡。
3.2知识技能收获
LLM“预训练-微调”全流程：完整地实践了从无到有（预训练）和从有到专（微调）的全过程，对现代NLP技术范式有了系统性的认识。
自监督学习与迁移学习：深刻理解了模型如何在无标签数据上进行自监督学习，以及如何将学到的知识通过迁移学习应用于有标签的下游任务。
PyTorch与Transformer应用：熟练掌握了使用PyTorch构建和训练Transformer模型的全过程。
3.3反思与感悟
本次实践最深刻的感悟在于分阶段学习的威力。模型首先通过预训练成为一个知识渊博的“通才”，然后通过微调快速转变为特定领域的“专家”。这一流程不仅极大地提升了AI模型的性能，更重要的是，它将模型训练的成本分摊开来——少数机构负责昂贵的预训练，而广大开发者和企业则可以低成本地进行微调和应用，这是推动当前AI技术浪潮的核心原因。
四、总结展望
4.1 项目成果总结
本项目完整地复现了现代大语言模型的“预训练-微调”范式。首先，通过在无标签文本上进行自回归语言建模，成功地从零开始预训练了一个小型GPT模型。随后，基于一个更强大的公开预训练模型GPT-2，通过参数高效微调策略，实现了一个高精度的垃圾短信分类器，其在测试集上准确率达到95.67%，并最终被封装为可交互的Web应用。
4.2不足与改进方向
预训练规模局限：预训练受限于数据集和计算资源，其规模较小。未来的研究可以探索在更大、更多样的语料上进行预训练，以获得更强的通用语言模型。
当前只采用了微调顶层模块的策略。未来可以进一步探索如LoRA、QLoRA等更先进的PEFT方法，以期在更少的训练参数下达到相似甚至更好的效果。
当前Web应用只是本地运行的。未来可以学习如何将模型部署到云服务器，通过API提供服务，使其成为一个真正可用的在线工具。

